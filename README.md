# A-Score-Regression-With-Gradient-Boosting-LGBM
A "simple" problem of regression where it is necessary to estimate a score from 300 variables that are totally unknown.

This problem is very simple to understand: there is a company that needs to estimate a credit card score for new potential clients. They have an historical database with 300 features where each row contains a lot of information from their clients. Due to security protocols, both sensitive data and field names were anonymized, so we will have to estimate the score with a blindfold on the eyes.

Let's get started!

# How to solve the problem

## 1.	Assumptions
-	The data come from a real phenomenon and are not generated by computer. Therefore, no random-data validation model has been run.
-	Although all data are numbers, it is assumed that not all data correspond to numeric variables. Based on this, as the exercise progresses, the decision is made to consider some variables as categorical. In this sense, any variable containing between 3 and 20 different values would be considered categorical. Otherwise, it is considered numeric (including the binary variables).
-	This is a regression problem. While the data for the variable 'y' are in the range [300,839] and there are only 540 different values out of 100.000 records, the low occurrence rate of some of these unique values is very low, making it difficult to approximate by classification. In addition, it is suggested to evaluate using RMSE, a classical metric for regression problems.

## 2.	Description of the methodology
As far as possible, the steps defined by the CRISP-DM methodology were implemented.
### 2.1.	Business Understanding
Without knowing the nature of the data, the origin, the business, or the purpose for which this solution is needed, it is impossible to make an understanding of the business. The present solution cannot be oriented to a practical approach for stakeholders. As in this case it is a merely technical exercise, for obvious reasons this stage of the methodology is not developed.
### 2.2.	Data Understanding & Data Preparation
Based on one of the main suppositions, the variables were identified as ‘categorical’ (from 3 to 20 unique values) and numerical (otherwise). The ‘x001’ is an ID and the ‘y’ variable is the label.
Basic statistics were calculated to look at the dispersion of the data (min, max, quartiles, mean, standard deviation, rho), as well as the number of different values (to determine the type of variable) and the amount of missing values.

| Variable | type | Type2 | missing | mean | std | rho | min | P25% | P50% | P75% | max | distinct values |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| x001 | ID | int64 | 0 | N/A | N/A | N/A | N/A | N/A | N/A | N/A | N/A | 100000 |
| x002 | numerical | int64 | 21432 | 125,7 | 115,8 | 0,9 | 0 | 32,0 | 100,0 | 180,0 | 718,0 | 666 |
| x003 | numerical | int64 | 21432 | 25,5 | 49,0 | 1,9 | 0 | 3,0 | 8,0 | 24,0 | 704,0 | 457 |
| x004 | numerical | int64 | 21424 | 65,4 | 63,6 | 1,0 | 0 | 19,0 | 48,0 | 92,0 | 704,0 | 492 |
| … | … | … | .. | … | … | … | … | … | … | … | … | … |
| x301 | binary | int64 | 0 | 0,1 | 0,3 | 2,9 | 0 | 0,0 | 0,0 | 0,0 | 1,0 | 2 |
| x302 | categorical | int64 | 73069 | 2,9 | 2,1 | 0,7 | 1 | 1,0 | 1,0 | 5,0 | 9,0 | 7 |
| x303 | numerical | int64 | 0 | 6410,9 | 24190,2 | 3,8 | 0 | 0,0 | 0,0 | 0,0 | 785537,0 | 15165 |
| x304 | numerical | float64 | 81875 | 1,0 | 0,3 | 0,3 | 0 | 1,0 | 1,0 | 1,2 | 5,2 | 8011 |
| y | numerical | int64 | 0 | 619,2 | 118,5 | 0,2 | 300 | 524,0 | 599,0 | 720,0 | 839,0 | 540 |

_**Table 1.** Example of the basic measures._

This dataset presents 42 variables with missing values. These variables are:
[__'x242','x295','x304','x098','x155','x259','x255','x256','x257','x302','x268','x162','x265','x266','x267','x253','x297','x275','x293'__,'x288','x289','x290','x148','x223','x222','x041','x057','x058','x237','x238','x239','x287','x002','x003','x004','x235','x044','x045','x234','x272','x005']

In __bold__, variables with more than 50% missing data.

In terms of dispersion, 182 numerical variables have a rho greater than 1, along with high accumulation of occurrences before Q1 and after Q3. When reviewing the categorical variables, cases are found such as, for example, feature 'x070' with a highly loaded distribution for a category (99.77% of the cases are equal to 0 and the rest is divided into three categories). All the above means that the data have a very high variability that can make it difficult to detect outliers.
As a special case, the variables 'x067', 'x094', 'x095' and 'x096' are totally useless, as they only have one value for all the rows.
Considering these characteristics, it is necessary then:

- Detect the most important variables and eliminate the least relevant ones.
    - A method was developed that combines partial correlations and information gain.
    - First, the partial correlation between the independent variables (X) is calculated to identify which ones are redundant. The variables with the greatest number of correlations with other variables (where each correlation is greater than r2=0.64) are eliminated.
    - Second, using the information gain technique (vital in decision tree algorithms) a score is obtained for each variable. The higher the score, the more relevant the feature. Any variable with information gain less than 0.15 is eliminated.
    - Finally, from the remaining variables are eliminated those that have an absolute correlation with the variable 'y' less than 0.02.
- Identify outliers and eliminate them.
    - A standardization function was used for numerical variables based on Zscore ((x-μ ̂)/σ ̂ ).
    - For each cell, this standardization was calculated and then the absolute value of each cell was averaged across all rows.
    - The rows with the highest averages (a proportion α% assigned by the analyst) were marked as outliers and then deleted.
- Transform numerical variables.
    - Transformations such as standardization (zscore), min-max and min-mean were considered.
- In the case of missing values, depending on the regression algorithm these must be filled. From experience, in cases where the 'NaN' are frequent it is better not to fill them because their absence may contain patterns and useful information for the models.
    - For this purpose -and considering the high computational cost of the imputation algorithms-, a clustering-based method was designed.
    - First, k-Means is used to divide the records into clusters, and then for each cluster a simple imputation is applied to the NaN replacing them with the most frequent value of each row within that cluster.
    - This reduces imputation time (compared against a kNN method, for example) and clustering allows assigning imputations in similar data groups.
- Transform categorical variables
    - Many algorithms such as decision trees or neural networks are not good at dealing with categorical variables, so it is necessary to binarize them.
    - ‘Dummies’ were obtained by separating each categorical variable into several binary variables. No category was eliminated for those variables with missing values.

### 2.3.	Modeling & testing
The process for generating the regression model is as follows (all steps are iterative, not all steps from (iii) to (vi) need to be completed):
- Remove ID and other useless variables.
- Divide the dataset into trainset and testset using holdout (usually 40% for testset).
- Transform categorical variables into dummies.
- Remove variables with low importance.
- Eliminate trainset outliers.
- Standardize numerical values.
- Train the model
- Evaluate the model (using RMSE)
    -	Calibrate parameters
    - Generate final model
Several options were considered for the regression algorithm. The nature of the data (with so many missing values) suggests that a good option are assemblies with decision trees (they are good at dealing with missing values, with extreme values and if properly calibrated are very robust against overfitting). The initial candidates are:
-	Linear Regression
-	Gradient boosting trees (Light Gradient Boosting , Extreme Gradient Boosting)
-	Random forests
-	Deep learning
To identify the best option, a Python library called 'auto_ml' was used to evaluate multiple models (optimizing the parameters). This library is only useful to know which type of algorithm is the best, but not to calibrate the model (because the developers designed it as a black box and it is not possible to identify the parameters). 'Auto_ml' connects with ‘sklearn’, ‘keras’, ‘xgboost’ and ‘lightgbm’.
Confirming the initial hypothesis, the best models were, in order, Light Gradient Boosting, Extreme Gradient Boosting and Random Forests.
Given that **Light Gradient Boosting** is the fastest to train between the three options, and that the difference in RMSE obtained is minimal versus the other two options, the model was further developed with this algorithm.

| Model | RMSE |
| --- | --- |
| LGBMRegressor | 29,4498 |
| RandomForestRegressor | 32,7371 |
| XGBRegressor | 33,5869 |
| BayesianRidge | 41,1235 |
| LinearRegression | 42,9569 |
| Lasso | 46,5700 |
| AdaBoostRegressor | 47,8637 |
| Ridge | 102,2082 |
| DeepLearningRegressor | 273,1137 |
| SGDRegressor | 2,20E+19 |

_**Table 2.** Best models according to auto_ml library (Python 3)._

In the next step, the parameters were calibrated. The 'lightgbm' library has a regression function containing more than 15 parameters. It was decided to do a pseudo-optimization in cascade, first setting initial values for the parameters and then optimizing the value of each parameter in an order predefined by the data scientist, reviewing the RMSE value obtained in each iteration. The “optimized” parameters that minimize the RMSE (in their respective order of evaluation) were:
[boosting_type, learning_rate, subsample, colsample_bytree, min_child_weight, min_child_samples, reg_lambda, reg_alpha]
The best iteration obtained RMSE = 25.31 for the test set and RMSE = 21.58 for the trainset. Accuracy based on an absolute error of 3 units: 18.34%. Personally, it is not a good result nor is it close to what is required in the problem, but at least it is highly robust against overfitting.
The final model included the following steps:
-	Remove ID and other useless variables. Implemented.
-	Transform categorical variables into dummies. Implemented.
-	Remove variables with low importance. Implemented.
-	Divide the dataset into trainset and testset using holdout (30% for testset). Implemented.
-	Eliminate trainset outliers (2.5% of the outliers). Implemented.
-	Impute missing values. Not implemented. After several tests the results with imputations were worse.
-	Standardize numerical values. Not implemented. After several tests the results with standardization were worse.
    -	Train the model using LightGBM.
    -	Evaluate the model (using RMSE). Best result: RMSE = 25.31.
        -	Calibrate parameters. Best parameters:
        -	n_estimators=2000 (with early stop after 8 epochs without improvement)
        -	min_child_weight=1.5
        -	learning_rate=0.1
        -	subsample=0.6
        -	colsample_bytree=0.5
        -	max_depth=8
        -	reg_lambda=0.5
        -	reg_alpha=0.25
        -	gamma=1

### 2.4.	Implementation
The best model was generated according to the given specifications.

**List of final algorithms and techniques**
-	Transform categorical variables to binary dummies.
-	Remove variables with low importance, using Gain Information, Correlations.
-	Divide dataset with holdout.
-	Eliminate outliers using zscores.
-	Train model using Light Gradient Boosting.
-	Obtain best parameters using cascade-“optimization”.

**List of tools and frameworks**
-	All the code was developed in Python 3.6 using the Google Colab tool in the cloud.
-	Specific libraries that must be installed in order to run the code:
    -	numpy
    -	pandas
    -	scikit-learn (sklearn)
    -	lightgbm
    -	math
    -	pickle
